## Bagging

در این روش از مدل های پایه همگون استفاده می شود، آنها به صورت مستقل از یکدیگر و به صورت موازی آموزش می بینند 
و با فرایند میانگین گیری قطعی (Deterministic averaging process) با یکدیگر ترکیب می شوند.

یا به عبرات دیگر، در روش bagging یک زیر مجموعه از مجموعه داده‌ی اصلی به هر کدام از طبقه‌بندها داده می‌شود. 
یعنی هر طبقه‌بند یک قسمت از مجموعه‌ی داده را مشاهده کرده و باید مدل خود را بر اساس همان قسمت از داده‌ها که در اختیارش قرار گرفته است، بسازد 
(یعنی کلِ دیتاست به هر کدام از طبقه‌بندها داده نمی‌شود).

💻در این کد، دقت روش برابر با 91 درصد بوده است.

----

## Boosting

در این روش نیز از مدلهای پایه همگون استفاده می شود که به صورت دنباله ایی و با یک روش تطبیقی (Adaptive) آموزش می بینند 
(به طوری که یک مدل پایه وابسته به مدل قبلی خود است) و با یک استراتژی قطعی ترکیب می شوند. در شکل زیر تفاوت روش bagging و boosting را مشاهده می کنید.

یا به عبارت دیگر، فرض کنید شما یک سری نمونه سوالِ امتحانی دارید. از ابتدا آن‌ها را همراه پاسخِ داده شده می‌خوانید و یاد می‌گیرید.
در حالِ خواندن آن‌هایی که برایتان مشکل‌تر است را علامت‌گذاری می‌کنید تا بعداً دوباره مرور کنید.
همین روش هم در boosting به کار گرفته می‌شود.

💻در این کد، دقت روش برابر با 95 درصد بوده است.

----

# Stacking

در این روش از مدل های پایه ناهمگون استفاده می شود که به صورت موازی آموزش می بینند و با آموزش یک متامدل (Meta-Model) بر
روش خروجی های پیش بینی شده مدل های پایه، ترکیب می شوند.

طبقه‌بندهای ترکیبی عموماً از overfit شدنِ مدلِ یادگرفته شده توسطِ الگوریتم، جلوگیری می‌کنند و در بسیاری از موارد نتایج بهتری نسبت به الگوریتم‌های دیگر تولید می‌کنند.

💻در این کد، دقت روش برابر با 98 درصد بوده است.
