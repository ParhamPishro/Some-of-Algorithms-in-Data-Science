## K-Means

در خوشه بندی K-Means ابتدا باید تعدادی نقطه تصادفی را به عنوان مرکز خوشه انتخاب کرده؛ سپس فاصله هر کدام از دیگر نقاط را با مرکز خوشه مقایسه کرده و
داده ها را با نزدیکترین مرکز خوشه ادغام کرد. سپس مراکز خوشه با توجه به خوشه های جدیدی که هم اکنون تولید شده اند، مقادیر جدیدی پیدا می کنند. و سپس
همانند مرحله قبل، دوباره داده ها باید با نزدیکترین مرکز خوشه موجود ادغام شوند. این مراحل تا جایی که دیگر مراکز خوشه جابجا نشوند، ادامه خواهد داشت.

💻 در این کد، ابتدا مثالی که در کلاس مطرح شده بود، نوشته شده. در واقع ابتدا یک سری نقاط تصادفی تولید شده و بعد در بین آن ها عمل خوشه بندی را انجام می دهیم.

پس از آن با داده های واقعی یعنی EscapeRoom این کار را تکرار خواهیم کرد. نکته ای که حائز اهمیت است، اینست که الگوریتم خوشه بندی K-Means برای داده های عددی مناسب
است. پس در ابتدا باید متغیر های عددی را انتخاب کرده و الگوریتم را روی آن متغیرها پیاده سازی کرد. 
پس از انتخاب متغیرهای عددی، حال درون یک حلقه for خوشه بندی را برای 2 الی 9 خوشه امتحان می کنیم. بهترین خوشه بندی، 2 خوشه دارد.

حال با کمک PCA بعد داده ها را به 2 کاهش داده و پس از آن، خوشه ها را نمایش می دهیم.

----

## DBScan

در الگوریتم DBScan با مشخص کردن یک شعاع مشخص و یک حداقل تعداد همسایه، عمل خوشه بندی انجام می شود. بدین صورت که از یک نقطه تصادفی موجود شروع کرده.
دایره ای به شعاع مفروض و مرکز آن نقطه زده می شود و اگر حداقل تعداد همسایه های مفروض درون دایره وجود داشت، آن نقاط به عنوان یک خوشه باهم ادغام می شوند.
حال باید همین روند را برای دیگر نقاطی که به خوشه ادغام شده اند انجام داد. مراحل فوق را برای همه داده ها باید انجام داد.

💻در این کد، ابتدا با کمک دو مثال کاربرد روش DBScan در مقایسه با روش K-Means نمایش داده شده است.

پس از آن، عمل خوشه بندی روی مجموعه داده واقعی EscapeRoom پیاده سازی شده و همانند کاری که در روش K-Means انجام شده، اینجا هم انجام می دهیم.
یعنی با کمک PCA ابعاد را برای نمایش داده ها کاهش می دهیم و پس از آن از الگوریتم DBScan استفاده خواهیم کرد. همانگونه که پیداست، الگوریتم DBScan بر روی این
مجموعه داده عملکرد خوبی نخواهد داشت.

----

## EM

### الگوریتم EM:
1. در گام اول (E-step) یا گام متوسط‌گیری، هدف برآورد توزیع متغیر پنهان به شرط وزن (π)، میانگین‌ها (Means) و ماتریس کوواریانس (Σ) توزیع آمیخته نرمال است. بردار پارامترها را در این جا با نماد θ نشان می‌دهیم. برای این کار ابتدا یک حدس اولیه برای این پارامترها زده می‌شود سپس گام‌های به ترتیب برداشته می‌شوند. همانطور که خواهید دید، حدس‌های اولیه را می‌توان بوسیله الگوریتم kmeans بدست آورد. به این معنی که برای خوشه‌های حاصل از الگوریتم kmeans، میانگین، ماتریس کوواریانس و وزن‌ها محاسبه می‌شود. توجه داشته باشید که منظور از وزن، درصدی (احتمال) از داده‌ها است که در یک خوشه قرار دارند.
2. در گام دوم (M-step) با استفاده از متغیرهای پنهان، سعی خواهیم کرد تابع درستنمایی را نسبت به پارامترهای θ بیشینه کنیم. این مراحل (گام E و گام M) تا زمانی که الگوریتم همگرا شود (مقدار تابع درستنمایی تغییر نکند)، ادامه خواهد داشت. به این ترتیب الگوریتم EM علاوه بر برآورد پارامترهای توزیع آمیخته گاوسی، برچسب‌ها یا مقدار متغیر پنهان را نیز مشخص می‌کند.


### مزیت خوشه بندی نرم به خوشه بندی سخت:
+ اول اینکه این الگوریتم می‌تواند خوشه هایی غیر کروی (دایره ای) را پیدا کند. همان طور که می‌دانید الگوریتم KMeans می‌تواند خوشه هایی کروی (دایره ای) را بیابد و این می‌تواند نقطه ضعف KMeans باشد، ولی در GMM با توجه به ساختار و توزیعِ گوسی عملیاتِ خوشه‌بندی انجام می‌شود.
+ همچنین در الگوریتم GMM هر نمونه (نقطه) می‌تواند به چند خوشه تعلق پیدا کند (به نسبت عضویت در ساخت توزیع گوسی آن خوشه). در واقع در GMM ما نوعی خوشه‌بندی نرم (Soft Clustering) داریم که هر نمونه (نقطه) می‌تواند به بیش از یک خوشه تعلق داشته باشد. ولی در KMeans خوشه‌بندی یک خوشه‌بندیِ سخت است (Hard Clustering)، به این معنی که هر نمونه (نقطه) فقط می‌تواند به یک خوشه تعلق داشته باشد. مثلا اگر در جایی می‌خواهید مشتریانِ یک فروشگاه را به خوشه‌های مختلف تقسیم کنید و یک مشتری می‌تواند به چند خوشه (گروه) تعلق داشته باشد، می‌توانید از این روش‌ها (الگوریتم‌های خوشه‌بندیِ نرم) استفاده کنید.

💻در این کد، ابتدا با ساختن یک مجموعه داده، تفاوت میان K-Means و EM را نمایش می دهیم. پس از آن هر دوی این روش های خوشه بندی را روی مجموعه داده ی واقعی 
امتحان می کنیم. با مصورسازی داده ها، تفاوت بین این دو روش واضح تر به نمایش گذاشته می شود.
