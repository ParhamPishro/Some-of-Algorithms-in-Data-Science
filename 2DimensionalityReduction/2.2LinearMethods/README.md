## Principal Component Analysis:

فرض کنید که مجموعه داده ما، nبعدی باشد (یعنی دارای nتا feature باشد) و هدف کاهش ابعاد این مجموعه به kبعد می باشد. روش کلی تحلیل مولفه های اصلی بدین صورت است که:

1. ابتدا ماتریس کوواریانس مجموعه داده ها را باید محاسبه کرد.
2. سپس مقادیر ویژه و بردارهای ویژه این ماتریس را محاسبه نمود.
3. سپس مقادیر ویژه را به ترتیب نزولی مرتب نمود و به دنبال آن بردارهای ویژه را به صورت سطری و متناظر با مقادیر ویژه درون ماتریسی قرار داد.
4. حال باید k بردار ویژه بالای ماتریس ساخت شده را (یعنی بردار ویژه هایی که بیشترین مقدار ویژه را دارند) انتخاب کرد و آن ها را درون ماتریس جدیدی به نام PCA قرار داد.
5. در نهایت با کمک ماتریس ساخته شده PCA می توان مجموعه داده ها را از nبعد به kبعد کاهش داد، در حالی که بیشترین میزان واریانس حفظ شود.


💻 در این کد، ابتدا 3 مولفه اصلی را با کمک دستور PCA پیدا کردیم. و متوجه شدیم که میزان نسبت واریانسی که اولین مولفه اصلی به ما باز میگرداند برابر 87 درصد می باشد.
مولفه اصلی دوم نیز 7 درصد و در نهایت مولفه اصلی سوم نیز 2 درصد واریانس کلی را شامل می شود. در ادامه می توان با جمع میزان نسبت واریانس ها متوجه شد که
دو مولفه اصلی اول و دوم، در مجموع حدود 94 درصد واریانس کلی را شامل می شوند. همچنین 3 مولفه اصلی اول نیز در مجموع 97 درصد واریانس کلی را پوشش می دهند.

در ادامه تعداد 9 مولفه اصلی (که برابر با همان تعداد feature های ماست) می یابیم و متوجه خواهیم شد که 3 سطر اول ماتریس PCA در واقع برابر با همان 3 سطری است که در
مرحله قبل به دست آورده ایم. به طور مشابه، میزان نسبت واریانس های آن ها نیز باهم برابر خواهند شد.

و در نهایت با کمک مدل شبکه عصبی که بر روی همه featureها و همچنین بر روی 2 مولفه اصلی اول اعمال شده؛ میزان دقت مدل را مقایسه می کنیم.

در حالت اول، یعنی با وجود همه featureها، میزان دقت مدل برابر با 90درصد بوده و در حالت دوم، یعنی فقط 2 مولفه اصلی اول، میزان دقت 72 درصد خواهد بود.

----

## Factor Analysis


تحلیل عاملی (Factor Analysis) روشی است برای کاهش ابعاد به صورتی که متغیرهای پنهان موجود در میان داده ها را با استفاده از متغیرهای قابل مشاهده (Feature)
پیدا کرده و آن متغیرهایی که بیشترین تغییرات را توجیه می کنند به عنوان عامل (Factor) در نظر گرفته می شوند.

تحلیل عاملی را می‌توان به صورت تبدیل متغیرهای مربوط به یک مدل از X1,X2,⋯,Xi به F1,F2,⋯,Fk در نظر گرفت که در واقع مقدار k بسیار کوچکتر از ‌i است.
### مفروضات:
+ عدم وجود داده ی پرت در میان داده ها
+ بزرگتر بودن اندازه نمونه نسبت به تعداد فاکتورها
+ عدم وجود همخطی کامل بین متغیرها (عدم وجود رابطه خطی کامل بین متغیرها یا همان عدم وجود همبستگی زیاد بین متغیرها)
+ عدم یکنواختی پراکندگی داده‌ها در بین متغیرها (متفاوت بودن توزیع متغیرها)

### انواع تحلیل عاملی:
+ روش «تحلیل عاملی اکتشافی» (Explanatory Factor Analysis) یا EFA که در آن هر متغیر قابل مشاهده ترکیب خطی از همه عوامل خواهد بود.
+ روش «تحلیل عاملی تاییدی» (Confirmatory Factor Analysis) یا CFA که در آن تنها ارتباط بین گروهی از متغیرهای قابل مشاهده و هر عامل در نظر گرفته می شود.

### روش تحلیل عاملی:
1. با استفاده از PCA، فاکتورها استخراج می شوند.
2. سپس به منظور تفسیرپذیری بهتر فاکتورها و همچنین جلوگیری از وابستگی (تعامد) بین عوامل، فاکتورهای ایجاد شده در مرحله قبل چرخانده می شوند.
3. در نهایت فاکتورهایی که سهم بیشتری از واریانس کل را بیان می کنند، به عنوان فاکتورهای نهایی در نظر گرفته می شود.

### مقایسه تحلیل عاملی و تحلیل مولفه های اساسی:
+ مولفه های PCA برهم عمودند در حالی که در FA لزومی بر عمود بودن فاکتورها وجود ندارد.
+ در PCA، هر کدام از Feature ها ترکیبی خطی از مولفه های اساسی هستند در حالی که در FA هر کدام از Feature ها ترکیبی خطی از فاکتورها هستند.
+ مولفه های اساسی PCA قابل تفسیر یا نام گذاری نیستند، در حالی که فاکتورهای موجود در FA را می توان تفسیر و نام گذاری کرد.
+ روش PCA، روشی برای کاهش بعد است با استفاده از مولفه‌هایی که بیشترین پراکندگی را بیان می‌کنند. FA برای تولید متغیرهای پنهان به کار می‌رود.

### انواع آزمون (جهت اطمینان از پیدا کردن فاکتور در بین داده ها) :
+ آزمون بارتلت (Bartlett's Test)
+ آزمون کایزر-مایر-اولکین (Kaiser-Meyer-Olkin Test)

💻 در این کد، ابتدا با کمک آزمون KMO میزان عامل پذیری مجموعه داده بررسی شده است؛ که با توجه به مقدار 0.64 که به دست آمده می توان متوجه شد که این مجموعه داده
عامل پذیر است (یعنی دارای فاکتورهای پنهان می باشد).
سپس مقدارهای ویژه را استخراج کرده و با کمک آن، نمودار Scree Plot را جهت انتخاب تعداد فاکتورها رسم می کنیم. با توجه به نمودار 5 فاکتور انتخاب می شود.

----

## Linear Discriminant Analysis

در این الگوریتم برخلاف PCA و FA، از متغیر هدف نیز استفاده می شود.
چرا که همواره تعداد Feature های ساخته شده در الگوریتم، یکی کمتر از دسته های متغیر هدف خواهد بود.
فرض هایی که برای الگوریتم LDA در نظر گرفته می شوند، شامل توزیع نرمال بودن همه Feature ها و همانی بودن ماتریس کوواریانس می باشد.
جهت همانی بودن ماتریس کوواریانس، باید واریانس هر Feature برابر 1 بوده و کوواریانس دو به دوی همه Featureها برابر با 0 باشد (یا به عبارت دیگر Featureها
نرمال استاندارد شده باشند).

اگر تعداد دسته های متغیر هدف 2 تا باشد، الگوریتم برداری را پیدا می کند که در آن فاصله بین میانگین هر دسته بیشترین (maximum) و واریانس هر دسته حداقل (minimum)
شود.

اگر تعداد دسته های متغیر هدف 3 تا باشد، الگوریتم صفحه ای را پیدا می کند که خاصیتی مشابه با بردار بالا داشته باشد.

اگر تعداد دسته های متغیر هدف 4 تا باشد، الگوریتم فضایی را پیدا می کند که خاصیتی مشابه با صفحه بالا داشته باشد.

و همینطور الی آخر...

----

## Singular Value Decomposition
